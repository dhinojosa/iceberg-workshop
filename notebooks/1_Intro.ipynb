{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56119d8-fd30-4ece-80db-a51a17561949",
   "metadata": {},
   "source": [
    "# Apache Iceberg in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a0218c-bcba-4bf3-bd55-0909ba6ffbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f086b397-6223-40a6-9634-cb9e0a0ab489",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Intro to Iceberg in Spark\")\n",
    "    .master(\"spark://spark:7077\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ef517f-20e2-4a66-84b2-4b4ac9ec5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "s3a://spark-events/logs/\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.eventLog.enabled\"))\n",
    "print(spark.conf.get(\"spark.eventLog.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d47f4c-86a9-490e-9af3-b7ac69c139fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9a9cc-60c1-4ec1-8682-ae00085fb6b7",
   "metadata": {},
   "source": [
    "The following is purely for debugging, but you may find it interesting, this is the configuration for our job. It shows the settings from our configuration file. One of the important aspects to note is the location of the iceberg repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a2259e-e5a3-4b55-a7eb-94bd41b7fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.eventLog.enabled = true\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Daws.region=us-east-1\n",
      "spark.hadoop.fs.s3a.connection.ssl.enabled = false\n",
      "spark.sql.catalog.ice.s3.secret-access-key = minioadmin\n",
      "spark.hadoop.fs.s3a.path.style.access = true\n",
      "spark.sql.defaultCatalog = ice\n",
      "spark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "spark.driver.port = 41637\n",
      "spark.driver.host = c820f10354d4\n",
      "spark.hadoop.fs.s3a.create.directory.marker = true\n",
      "spark.hadoop.fs.s3a.access.key = minioadmin\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.app.startTime = 1761583569539\n",
      "spark.sql.catalog.ice.s3.access-key-id = minioadmin\n",
      "spark.submit.deployMode = client\n",
      "spark.sql.warehouse.dir = file:/home/jovyan/work/spark-warehouse\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Daws.region=us-east-1\n",
      "spark.eventLog.dir = s3a://spark-events/logs/\n",
      "spark.hadoop.fs.s3a.secret.key = minioadmin\n",
      "spark.app.name = Intro to Iceberg in Spark\n",
      "spark.sql.catalog.ice.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.ice.uri = http://iceberg-rest:8181\n",
      "spark.master = spark://spark:7077\n",
      "spark.executor.id = driver\n",
      "spark.sql.catalog.ice.warehouse = s3a://warehouse/\n",
      "spark.sql.catalog.ice.s3.endpoint = http://minio:9000\n",
      "spark.sql.catalog.ice = org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.catalog.ice.s3.region = us-east-1\n",
      "spark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "spark.sql.catalog.ice.type = rest\n",
      "spark.app.id = app-20251027164610-0000\n",
      "spark.rdd.compress = True\n",
      "spark.app.submitTime = 1761583569437\n",
      "spark.hadoop.fs.AbstractFileSystem.s3a.impl = org.apache.hadoop.fs.s3a.S3A\n",
      "spark.submit.pyFiles = \n",
      "spark.hadoop.fs.s3a.endpoint = http://minio:9000\n",
      "spark.history.fs.logDirectory = s3a://spark-events/logs/\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.sql.catalog.ice.s3.path-style-access = true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print all key-value pairs in Spark config\n",
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} = {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0d9f41-8123-4b27-acee-1dfd2b825fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://spark:7077\n",
      "http://c820f10354d4:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.master) # should be spark://spark:7077\n",
    "print(spark.sparkContext.uiWebUrl) # link to the app UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a72b252-0aa9-4e7c-bca8-eef22eadd906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN ice\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f4bd83-2067-4759-9f8b-63bbbe62fd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS ice.demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fc7873-9d13-4447-ac56-316801bc4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|demo     |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN ice\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a09cfe-a638-4774-97fc-213ca45c8260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ice.demo.customers (\n",
    "        id BIGINT,\n",
    "        name STRING,\n",
    "        email STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (email)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d92ee4-3a6b-4a6f-b7c9-662eba80da22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO ice.demo.customers VALUES\n",
    "      (1, 'Alice Smith', 'alice@example.com'),\n",
    "      (2, 'Bob Johnson', 'bob@example.com'),\n",
    "      (3, 'Carol Adams', 'carol@example.com')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba57b3-dde5-4864-b027-38451fe8bdff",
   "metadata": {},
   "source": [
    "Select the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3bd04c-b673-4c11-88e0-6b0a73994978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------+\n",
      "| id|       name|            email|\n",
      "+---+-----------+-----------------+\n",
      "|  3|Carol Adams|carol@example.com|\n",
      "|  1|Alice Smith|alice@example.com|\n",
      "|  2|Bob Johnson|  bob@example.com|\n",
      "+---+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.demo.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5949c-9655-411b-b3ed-af117e01e1ad",
   "metadata": {},
   "source": [
    "Select the customers with an `o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc77508-ccf5-4467-a8ad-616210af43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------+\n",
      "| id|       name|            email|\n",
      "+---+-----------+-----------------+\n",
      "|  3|Carol Adams|carol@example.com|\n",
      "|  2|Bob Johnson|  bob@example.com|\n",
      "+---+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.demo.customers WHERE name like '%o%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b250d6f5-829d-4360-8f6e-6531d9fe845d",
   "metadata": {},
   "source": [
    "Let's add some more data to our DataLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ecb9f4c-7ca4-49ee-b1b9-bbb43fd62814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO ice.demo.customers VALUES\n",
    "      (4,  'Diego Ramirez',       'diego.ramirez@example.com'),\n",
    "      (5,  'Maya Patel',          'maya.patel@example.com'),\n",
    "      (6,  'Liam O’Connor',       'liam.oconnor@example.com'),\n",
    "      (7,  'Sofia Almeida',       'sofia.almeida@example.com'),\n",
    "      (8,  'Noah Williams',       'noah.williams@example.com'),\n",
    "      (9,  'Ava Thompson',        'ava.thompson@example.com'),\n",
    "      (10, 'Ethan Chen',          'ethan.chen@example.com'),\n",
    "      (11, 'Olivia Garcia',       'olivia.garcia@example.com'),\n",
    "      (12, 'Lucas Martin',        'lucas.martin@example.com'),\n",
    "      (13, 'Emma Robinson',       'emma.robinson@example.com'),\n",
    "      (14, 'Benjamin Kim',        'benjamin.kim@example.com'),\n",
    "      (15, 'Isabella Rossi',      'isabella.rossi@example.com'),\n",
    "      (16, 'James Nguyen',        'james.nguyen@example.com'),\n",
    "      (17, 'Mila Novak',          'mila.novak@example.com'),\n",
    "      (18, 'Henry Scott',         'henry.scott@example.com'),\n",
    "      (19, 'Aria Johnson',        'aria.johnson@example.com'),\n",
    "      (20, 'Daniela Costa',       'daniela.costa@example.com'),\n",
    "      (21, 'Jack Wilson',         'jack.wilson@example.com'),\n",
    "      (22, 'Zoe King',            'zoe.king@example.com'),\n",
    "      (23, 'Oliver Brown',        'oliver.brown@example.com')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5284170-fec8-420e-80c7-2578aca0c5c4",
   "metadata": {},
   "source": [
    "Let's run our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd099c5-e0f7-4d86-a551-1f377e9e2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|          name|               email|\n",
      "+---+--------------+--------------------+\n",
      "| 22|      Zoe King|zoe.king@example.com|\n",
      "| 21|   Jack Wilson|jack.wilson@examp...|\n",
      "| 15|Isabella Rossi|isabella.rossi@ex...|\n",
      "| 18|   Henry Scott|henry.scott@examp...|\n",
      "|  7| Sofia Almeida|sofia.almeida@exa...|\n",
      "| 19|  Aria Johnson|aria.johnson@exam...|\n",
      "| 20| Daniela Costa|daniela.costa@exa...|\n",
      "|  4| Diego Ramirez|diego.ramirez@exa...|\n",
      "|  8| Noah Williams|noah.williams@exa...|\n",
      "| 17|    Mila Novak|mila.novak@exampl...|\n",
      "|  9|  Ava Thompson|ava.thompson@exam...|\n",
      "| 13| Emma Robinson|emma.robinson@exa...|\n",
      "|  6| Liam O’Connor|liam.oconnor@exam...|\n",
      "| 23|  Oliver Brown|oliver.brown@exam...|\n",
      "|  3|   Carol Adams|   carol@example.com|\n",
      "|  2|   Bob Johnson|     bob@example.com|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.demo.customers WHERE name like '%o%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4ca027-05ab-432e-b24d-4940840ff73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
